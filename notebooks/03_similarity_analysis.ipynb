{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenifer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/paper_title_abstract.json', 'r') as f:\n",
    "    papers_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_title': 'Dynamic Network Model from Partial Observations',\n",
       " 'abstract': 'Can evolving networks be inferred and modeled without directly observing\\ntheir nodes and edges? In many applications, the edges of a dynamic network\\nmight not be observed, but one can observe the dynamics of stochastic cascading\\nprocesses (e.g., information diffusion, virus propagation) occurring over the\\nunobserved network. While there have been efforts to infer networks based on\\nsuch data, providing a generative probabilistic model that is able to identify\\nthe underlying time-varying network remains an open question. Here we consider\\nthe problem of inferring generative dynamic network models based on network\\ncascade diffusion data. We propose a novel framework for providing a\\nnon-parametric dynamic network model--based on a mixture of coupled\\nhierarchical Dirichlet processes-- based on data capturing cascade node\\ninfection times. Our approach allows us to infer the evolving community\\nstructure in networks and to obtain an explicit predictive distribution over\\nthe edges of the underlying network--including those that were not involved in\\ntransmission of any cascade, or are likely to appear in the future. We show the\\neffectiveness of our approach using extensive experiments on synthetic as well\\nas real-world networks.',\n",
       " 'main_collection_area': 'Natural Language Processing',\n",
       " 'github_repo': 'No GitHub link available'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(papers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_collection_area\n",
       "Computer Vision                24874\n",
       "Natural Language Processing     7863\n",
       "Graphs                          7017\n",
       "Reinforcement Learning          2516\n",
       "Sequential                       424\n",
       "Audio                             90\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['main_collection_area'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(text_list):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(text_list)\n",
    "    cosine_sim = cosine_similarity(vectors)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence-BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenifer/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def compute_sentence_embeddings(text_list):\n",
    "    return model_bert.encode(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI CLIP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def compute_clip_embeddings(text_list):\n",
    "    inputs = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model_clip.get_text_features(**inputs)\n",
    "    return outputs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "titles = df['paper_title'].tolist()\n",
    "abstracts = df['abstract'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_tfidf = compute_tfidf(titles)\n",
    "abstracts_tfidf = compute_tfidf(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_titles = compute_sentence_embeddings(titles)\n",
    "sentence_embeddings_abstracts = compute_sentence_embeddings(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_embeddings_titles = compute_clip_embeddings(titles)\n",
    "clip_embeddings_abstracts = compute_clip_embeddings(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce dimensionality for the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(embeddings, method='pca'):\n",
    "    if method == 'pca':\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced = pca.fit_transform(embeddings)\n",
    "    elif method == 'tsne':\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        reduced = tsne.fit_transform(embeddings)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduce_dimensionality' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reduced_titles_tfids \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_dimensionality\u001b[49m(titles_tfidf)\n\u001b[1;32m      2\u001b[0m reduced_abstracts_tfidf \u001b[38;5;241m=\u001b[39m reduce_dimensionality(abstracts_tfidf)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reduce_dimensionality' is not defined"
     ]
    }
   ],
   "source": [
    "reduced_titles_tfids = reduce_dimensionality(titles_tfidf)\n",
    "reduced_abstracts_tfidf = reduce_dimensionality(abstracts_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings_titles = reduce_dimensionality(sentence_embeddings_titles)\n",
    "reduced_embeddings_abstracts = reduce_dimensionality(sentence_embeddings_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_clip_titles = reduce_dimensionality(clip_embeddings_titles)\n",
    "reduced_clip_abstracts = reduce_dimensionality(clip_embeddings_abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(embeddings, color_by, title, color_map):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Convert color_by into a categorical type and get unique categories\n",
    "    categories = pd.Categorical(color_by)\n",
    "    category_codes = categories.codes\n",
    "    category_labels = categories.categories\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = plt.scatter(embeddings[:, 0], embeddings[:, 1], c=category_codes, cmap=color_map)\n",
    "\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Create a custom legend\n",
    "    unique_categories = np.unique(category_codes)\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=scatter.cmap(scatter.norm(code)), markersize=10) \n",
    "                       for code in unique_categories]\n",
    "    plt.legend(legend_elements, category_labels, title=\"Category\", loc=\"upper right\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot TF-IDF Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(reduced_titles_tfids, df['area'], 'TF-IDF Embeddings (Titles) - Colored by Area', 'plasma')\n",
    "plot_embeddings(reduced_abstracts_tfidf, df['area'], 'TF-IDF Embeddings (Abstracts) - Colored by Area', 'plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Sentence-BERT Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(reduced_embeddings_titles, df['area'], 'Sentence-BERT Embeddings (Titles) - Colored by Area', 'plasma')\n",
    "plot_embeddings(reduced_embeddings_abstracts, df['area'], 'Sentence-BERT Embeddings (Abstracts) - Colored by Area', 'plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CLIP Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduced_clip_titles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_embeddings(\u001b[43mreduced_clip_titles\u001b[49m, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLIP Embeddings (Titles) - Colored by Area\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplasma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m plot_embeddings(reduced_clip_abstracts, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLIP Embeddings (Abstracts) - Colored by Area\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplasma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reduced_clip_titles' is not defined"
     ]
    }
   ],
   "source": [
    "plot_embeddings(reduced_clip_titles, df['area'], 'CLIP Embeddings (Titles) - Colored by Area', 'plasma')\n",
    "plot_embeddings(reduced_clip_abstracts, df['area'], 'CLIP Embeddings (Abstracts) - Colored by Area', 'plasma')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
