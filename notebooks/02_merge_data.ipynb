{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/filtered-papers-with-abstracts.json', 'r') as f:\n",
    "    papers_data = json.load(f)\n",
    "\n",
    "with open('data/pwc/links-between-papers-and-code.json', 'r') as f:\n",
    "    links_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_url': 'https://paperswithcode.com/paper/dynamic-network-model-from-partial',\n",
       " 'arxiv_id': '1805.10616',\n",
       " 'title': 'Dynamic Network Model from Partial Observations',\n",
       " 'abstract': 'Can evolving networks be inferred and modeled without directly observing\\ntheir nodes and edges? In many applications, the edges of a dynamic network\\nmight not be observed, but one can observe the dynamics of stochastic cascading\\nprocesses (e.g., information diffusion, virus propagation) occurring over the\\nunobserved network. While there have been efforts to infer networks based on\\nsuch data, providing a generative probabilistic model that is able to identify\\nthe underlying time-varying network remains an open question. Here we consider\\nthe problem of inferring generative dynamic network models based on network\\ncascade diffusion data. We propose a novel framework for providing a\\nnon-parametric dynamic network model--based on a mixture of coupled\\nhierarchical Dirichlet processes-- based on data capturing cascade node\\ninfection times. Our approach allows us to infer the evolving community\\nstructure in networks and to obtain an explicit predictive distribution over\\nthe edges of the underlying network--including those that were not involved in\\ntransmission of any cascade, or are likely to appear in the future. We show the\\neffectiveness of our approach using extensive experiments on synthetic as well\\nas real-world networks.',\n",
       " 'url_abs': 'http://arxiv.org/abs/1805.10616v4',\n",
       " 'url_pdf': 'http://arxiv.org/pdf/1805.10616v4.pdf',\n",
       " 'proceeding': 'NeurIPS 2018 12',\n",
       " 'authors': ['Elahe Ghalebi',\n",
       "  'Baharan Mirzasoleiman',\n",
       "  'Radu Grosu',\n",
       "  'Jure Leskovec'],\n",
       " 'tasks': ['Open-Ended Question Answering'],\n",
       " 'date': '2018-05-27',\n",
       " 'methods': [{'name': 'ooJpiued',\n",
       "   'full_name': 'ooJpiued',\n",
       "   'description': 'Please enter a description about the method here',\n",
       "   'introduced_year': 2000,\n",
       "   'source_url': 'http://arxiv.org/abs/1805.10616v4',\n",
       "   'source_title': 'Dynamic Network Model from Partial Observations',\n",
       "   'code_snippet_url': None,\n",
       "   'main_collection': {'name': 'Language Models',\n",
       "    'description': '**Language Models** are models for predicting the next word or character in a document. Below you can find a continuously updating list of language models.\\r\\n\\r\\n',\n",
       "    'parent': None,\n",
       "    'area': 'Natural Language Processing'}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get README from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch the raw README file directly from the GitHub repository\n",
    "def fetch_raw_readme(repo_url):\n",
    "    try:\n",
    "        # Construct the base URL for the raw files\n",
    "        repo_name = repo_url.replace(\"https://github.com/\", \"\")\n",
    "        \n",
    "        # Possible README filenames with different capitalizations\n",
    "        possible_readme_files = [\n",
    "            \"README.md\", \"Readme.md\", \"readme.md\", \n",
    "            \"README.MD\", \"ReadMe.md\", \"readMe.md\", \n",
    "            \"README\", \"Readme\", \"readme\"\n",
    "        ]\n",
    "\n",
    "        for readme_file in possible_readme_files:\n",
    "            raw_readme_url = f\"https://raw.githubusercontent.com/{repo_name}/main/{readme_file}\"\n",
    "            response = requests.get(raw_readme_url)\n",
    "            \n",
    "            # Check for successful response\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "\n",
    "            # If \"main\" branch doesn't exist, try the \"master\" branch\n",
    "            raw_readme_url = f\"https://raw.githubusercontent.com/{repo_name}/master/{readme_file}\"\n",
    "            response = requests.get(raw_readme_url)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "        \n",
    "        print(f\"README not found for {repo_url}\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching README from {repo_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the merged data\n",
    "merged_data = []\n",
    "\n",
    "# Create a dictionary to map GitHub repos to papers using the paper URL\n",
    "github_links_dict = {link['paper_url']: link['repo_url'] for link in links_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each paper in the papers_with_abstracts.json file\n",
    "for paper in papers_data:\n",
    "    paper_url = paper['paper_url']\n",
    "    \n",
    "    # Extract the relevant fields\n",
    "    paper_title = paper.get('title', '')\n",
    "    abstract = paper.get('abstract', '')\n",
    "    github_link = github_links_dict.get(paper_url, 'No GitHub link available')\n",
    "\n",
    "    main_collection_area = None\n",
    "    for method in paper['methods']:\n",
    "        if 'main_collection' in method:\n",
    "            if method['main_collection'] and 'area' in method['main_collection']:\n",
    "                main_collection_area = method['main_collection']['area']\n",
    "                break\n",
    "\n",
    "    # Try to scrape the README content from the GitHub repository\n",
    "    #readme_content = None\n",
    "    #if github_link != 'No GitHub link available':\n",
    "    #    readme_content = fetch_raw_readme(github_link)\n",
    "    #    if readme_content is None:\n",
    "    #        readme_content = 'README not available'\n",
    "\n",
    "    # Add the merged data to the list\n",
    "    merged_data.append({\n",
    "        'paper_title': paper_title,\n",
    "        'abstract': abstract,\n",
    "        'main_collection_area': main_collection_area,\n",
    "        'github_repo': github_link,\n",
    "        #'github_readme_content': readme_content,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(merged_data)\n",
    "\n",
    "json_filename = 'paper_title_abstract.json'\n",
    "\n",
    "data_dict = df.to_dict(orient='records')\n",
    "with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
